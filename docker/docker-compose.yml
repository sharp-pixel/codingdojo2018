version: '3.4'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:5.0.0
    # ports:
    #   - 2181:2181
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      KAFKA_JMX_PORT: 9000
      ZOOKEEPER_SERVER_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9000"
    healthcheck:
      test: jps | grep QuorumPeerMain || exit 1
      interval: 300s
      timeout: 10s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - zookeeper

  kafka-broker:
    image: confluentinc/cp-kafka:5.0.0
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka-broker:9092'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 # For Exactly once with ONE broker
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 # For Exactly once with ONE broker
      KAFKA_JMX_PORT: 9000
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9000"
    healthcheck:
      test: jps | grep SupportedKafka || exit 1
      interval: 300s
      timeout: 10s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - kafka
      - zookeeper

  kafka-client:
    image: confluentinc/cp-kafka:5.0.0
    depends_on:
      - kafka-broker
    environment:
      # Dummy values to satisfy image requirements
      KAFKA_ZOOKEEPER_CONNECT: ignored
      KAFKA_ADVERTISED_LISTENERS: ignored
    command:
      "bash -c -a 'echo Waiting for Kafka to be ready... && \
                       /etc/confluent/docker/configure && \
                       cub kafka-ready -b kafka-broker:9092 1 60 --config /etc/kafka/kafka.properties && \
                       sleep 5 && 
                       kafka-topics --zookeeper zookeeper:2181 --topic events --create --partitions 3 --replication-factor 1 && \
                       kafka-topics --zookeeper zookeeper:2181 --topic users --create --partitions 3 --replication-factor 1 --config cleanup.policy=compact'"
    networks:
      - kafka
      - zookeeper

  schema-registry:
    image: confluentinc/cp-schema-registry:5.0.0
    depends_on:
      - kafka-broker
    ports:
      - 8081:8081
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'PLAINTEXT://kafka-broker:9092'
      KAFKA_JMX_PORT: 9000
      SCHEMA_REGISTRY_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9000"
    # healthcheck:
    #   test: curl -f schema-registry:8081/subjects || exit 1
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - kafka

  connect-worker:
    image: confluentinc/cp-kafka-connect:5.0.0
    depends_on:
      - zookeeper
      - kafka-broker
      - schema-registry
    ports:
      - 8083:8083
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka-broker:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      CONNECT_PLUGIN_PATH: '/usr/share/java'
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
      KAFKA_JMX_PORT: 9000
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9000"
    # healthcheck:
    #   test: curl -f kafka-connect:8083/connectors || exit 1
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    volumes:
      - ./mysql-connector-java-8.0.12.jar:/usr/share/java/kafka-connect-jdbc/mysql-connector-java-8.0.12.jar

    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - kafka
      - zookeeper
      - elastic
      - mysql
  #     - cassandra

  # cassandra:
  #   image: datastax/dse-server:6.0.1
  #   environment:
  #     DS_LICENSE: accept
  #   cap_add:
  #     - IPC_LOCK
  #   ulimits:
  #     memlock: -1
  #   ports:
  #     - 9042:9042
  #   # healthcheck:
  #   #   test: ["CMD-SHELL", "[ $$(nodetool statusgossip) = running ]"]
  #   #   interval: 30s
  #   #   timeout: 10s
  #   #   retries: 5
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "200k"
  #       max-file: "10"
  #   networks:
  #     - cassandra

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0
    environment:
      discovery.type: 'single-node'
      bootstrap.memory_lock: 'true'
      ES_JAVA_OPTS: '-Xms512m -Xmx512m'
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - 9200:9200
    # healthcheck:
    #   test: curl -f elasticsearch:9200 || exit 1
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - elastic

  kibana:
    image: docker.elastic.co/kibana/kibana:6.4.0
    depends_on:
      - elasticsearch
    ports:
      - 5601:5601
    # healthcheck:
    #   test: curl -f kibana:5601 || exit 1
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - elastic

  lenses:
    image: landoop/lenses:2.1.4
    depends_on:
      - zookeeper
      - kafka-broker
      - schema-registry
      - connect-worker
    environment:
      LENSES_PORT: 9991
      LENSES_KAFKA_BROKERS: "PLAINTEXT://kafka-broker:9092"
      LENSES_ZOOKEEPER_HOSTS: |
        [
          {url:"zookeeper:2181", jmx:"zookeeper:9000"}
        ]
      LENSES_SCHEMA_REGISTRY_URLS: |
        [
          {url:"http://schema-registry:8081", jmx:"schema-registry:9000"}
        ]
      LENSES_CONNECT_CLUSTERS: |
        [
          {
            name:"cluster_a",
            urls: [
              {url:"http://connect-worker:8083", jmx:"connect-worker:9000"}
            ],
            statuses:"connect-status",
            configs:"connect-configs",
            offsets:"connect-offsets"
          }
        ]
      LENSES_SECURITY_MODE: BASIC
      # Secrets can also be passed as files. Check _examples/
      LENSES_SECURITY_GROUPS: |
        [
          {"name": "adminGroup", "roles": ["admin", "write", "read"]},
          {"name": "readGroup",  "roles": ["read"]}
        ]
      LENSES_SECURITY_USERS: |
        [
          {"username": "admin", "password": "admin", "displayname": "Lenses Admin", "groups": ["adminGroup"]},
          {"username": "read", "password": "read", "displayname": "Read Only", "groups": ["readGroup"]}
        ]
    ports:
      - 9991:9991
      - 9102:9102
    # healthcheck:
    #   test: curl -f lenses:9991 || exit 1
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    volumes:
      - ./landoop-license.json:/license.json
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - kafka
      - zookeeper

  mysql:
    image: mysql
    environment:
      MYSQL_USER: dojo
      MYSQL_PASSWORD: dojo2018
      MYSQL_ROOT_PASSWORD: dojo2018
    networks:
      - mysql

  # neo4j:
  #   image: neo4j:3.4
  #   environment:
  #     NEO4J_AUTH: none
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "200k"
  #       max-file: "10"
  #   ports:
  #     - 7474:7474

networks:
  elastic:
  kafka:
  #cassandra:
  zookeeper:
  mysql:
